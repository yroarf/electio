import streamlit as st
import trafilatura
from urllib.parse import urljoin, urlparse
from groq import Groq
import os
import pandas as pd
from lxml import html
from trafilatura import html2txt
from bs4 import BeautifulSoup
import json
import matplotlib.pyplot as plt
import re
from groq.types.chat import ChatCompletionUserMessageParam
from datetime import datetime, date

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ========================= CONFIGURA√á√ÉO P√ÅGINA =========================

st.set_page_config(
    page_title=" Analisador de Conformidade",
    page_icon="üó≥Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ========================= VALIDA√á√ÉO DA CHAVE DO GROQ============ (checado)
if "GROQ_API_KEY" not in st.session_state:
    api_key = st.secrets.get("GROQ_API_KEY") or os.getenv("GROQ_API_KEY")
    if not api_key:
        st.error("Chave da API do Groq n√£o encontrada. Configure em secrets ou vari√°vel de ambiente.")
        st.stop()
    st.session_state.GROQ_API_KEY = api_key

client = Groq(api_key=st.session_state.GROQ_API_KEY)

# ========================= LISTA DE MODELOS DE IA DO GROQ========= (ok)

# alguns modelos que est√£o dispon√≠ves no Groq
GROQ_MODELS = [
    "llama-3.3-70b-versatile",
    "mixtral-8x7b-32768",
    "openai/gpt-oss-120b"
]

# ======================= LISTA DE CAMINHOS IRRELEVANTES ============= (ok)

LISTA_1 = [
    '/login', '/cadastro', '/conta', '/privacidade',
    '/contato', '/sobre', '/equipe', '/assinatura',
    '/webmail', '/galeria', '/simbolos'
          ]

col_titulo, col_data = st.columns(2)
with col_titulo:
    st.title("üó≥Ô∏è Analisador de Conformidade Normativa")
with col_data:
    st.markdown("**Data de refer√™ncia**")
    data_referencia = st.date_input(
        label="Per√≠odo eleitoral de refer√™ncia",
        value=None,  # sem valor padr√£o fixo ‚Üí usu√°rio deve escolher
        min_value=None,
        max_value=None,
        help="Selecione a data do primeiro turno).",
        format="DD/MM/YYYY"
    )
if data_referencia is not None:
    st.session_state.data_referencia = data_referencia
    st.caption(f"Data selecionada: **{data_referencia.strftime('%d/%m/%Y')}**")
else:
    st.session_state.data_referencia = None
    col_espaco, colAtivacaoDATA =st.columns(2)
    with colAtivacaoDATA:
        st.info("Selecione uma data de refer√™ncia para ativar a an√°lise contextualizada no per√≠odo eleitoral.")

st.markdown("**Compare conte√∫do de not√≠cias de sites institucionais com normas eleitorais**")

st.divider()
# ========================================================================================
#                            Configura√ß√µes do Modelo de IA
# ========================================================================================
if "modeloIA" not in st.session_state:
    st.session_state.modeloIA = GROQ_MODELS[0]

st.markdown("### IA (LLM)")
with st.expander("ü§ñ **Configura√ß√µes do Modelo de IA**", expanded=False):
    col_model1, col_model2 = st.columns(2)
    with col_model1:
        modeloIA = st.selectbox(
            "Selecione o Modelo de IA (Groq)",
            options=GROQ_MODELS,
            index=0
        )
        modeloIA = st.session_state.modeloIA
    with col_model2:
        max_links = st.slider("N√∫mero m√°ximo de LINKS por URL", 1, 20, 5, help="Quantos links internos seguir por site")
    temperatura = st.slider("Temperatura (criatividade)", 0.0, 2.0, 0.1, 0.1, help="O valor 0.0 √© determin√≠stico")

st.divider()

# ========================================================================================
#                                Adi√ß√£o e Lista de Sites (ok)
# ========================================================================================

st.markdown("### Adi√ß√£o de Sites")

if "sites_df" not in st.session_state:
    st.session_state.sites_df = pd.DataFrame(columns=["URL", "Nome do Site"])

# ====================== ADI√á√ÉO DE NOVO SITE ======================
def extrair_subdominio_gov(url: str) -> str:

    parsed = urlparse(url.strip())
    netloc = parsed.netloc.lower()

    if ':' in netloc:
        netloc = netloc.split(':')[0]
    if netloc.startswith('www.'):
        netloc = netloc[4:]
    if not netloc.endswith('.gov.br'):
        raise ValueError(f"A URL n√£o termina com .gov.br: {url}")
    dominio_sem_gov = netloc[:-7]
    partes = dominio_sem_gov.split('.')
    if len(partes) >= 2:
        resultado = '.'.join(partes[-2:])
    else:
        resultado = partes[-1]
    return resultado


with st.expander("üåê sites", expanded=False):
    st.markdown("##### Adicionar novo site")
    col1, col2 = st.columns([3, 1])
    with col1:
        nova_url = st.text_input(
            "URL do site (ex: https://www.municipio.uf.gov.br/noticias)",
            placeholder="https://www.exemplo.go.gov.br/noticias -* https:// *- √© mandat√≥rio",
            help="P√°gina principal de not√≠cias ou comunicados da administra√ß√£o p√∫blica."
        )

    if st.button("Adicionar Site", type="primary"):
        if not nova_url.strip():
            st.error("Por favor, insira uma URL v√°lida.")
        else:
            url_limpa = nova_url.strip().rstrip("/")
            urls_existentes = st.session_state.sites_df["URL"].str.rstrip("/").tolist()

            if url_limpa in urls_existentes:
                st.error("Esta URL j√° foi adicionada.")
            else:
                nome_exibicao = urlparse(url_limpa).netloc
                novo_site = pd.DataFrame([{
                    "URL": url_limpa,
                    "Nome do Site": nome_exibicao
                }])
                st.session_state.sites_df = pd.concat(
                    [st.session_state.sites_df, novo_site],
                    ignore_index=True
                )
                st.success(f"Site adicionado: {nome_exibicao}")
                st.rerun()

# ====================== LISTA EDIT√ÅVEL DE SITES ======================
    st.markdown("##### Lista de Sites para An√°lise")

    if st.session_state.sites_df.empty:
        st.info("Nenhum site adicionado ainda. Use o campo acima para incluir.")
    else:
        # Data editor com valida√ß√£o de duplicatas
        edited_df = st.data_editor(
            st.session_state.sites_df,
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                "URL": st.column_config.TextColumn(
                    "URL",
                    required=True,
                    help="URL completa da p√°gina de not√≠cias"
                ),
                "Nome do Site": st.column_config.TextColumn(
                    "Nome do Site",
                    required=False,
                    help="Nome amig√°vel para exibi√ß√£o"
                )
            },
            hide_index=True
        )

        # Valida√ß√£o: impedir URLs duplicadas ao editar
        urls_editadas = edited_df["URL"].str.strip().str.rstrip("/").tolist()
        if len(urls_editadas) != len(set(urls_editadas)):
            st.error("‚ö†Ô∏è Aten√ß√£o: N√£o √© permitido ter URLs duplicadas na lista.")
        else:
            # S√≥ atualiza o estado se n√£o houver duplicatas
            st.session_state.sites_df = edited_df
            st.success("Lista atualizada com sucesso!")
        # print(edited_df)
        st.caption(f"Total de sites: **{len(st.session_state.sites_df)}**")


# ==========================================================================##
#  ---------------------- IN√çCIO BASE LEGAL ---------------------------------

# """
# Esse trecho do c√≥digo √© dedicado ao carregamento da normatiza√ß√£o aplic√°vel.
# A estrutura separada visa dimininuir a lat√™ncia e reduzir a quantidade de tokens
# utilizados.
# A base de dados √© trabalhada no mesmo ambiente de an√°lise visando estabelecer
# uma conex√£o com o prompt de an√°lise de conformidade dos conte√∫dos dos sites
# """



@st.cache_data(ttl=3600)
def resumir_base_legal(base_legal: str, data_referencia: str, modeloIA: str) -> str:
    if not base_legal.strip():
        return "Nenhuma base legal fornecida."

    prompt_base_legal = f"""
    Voc√™ √© um jurista especializado em Direito Eleitoral.

    Dada a base legal completa abaixo e considerando a data de refer√™ncia do pleito \"\"\"{data_referencia}\"\"\",

    Gere uma an√°lise ESTRUTURADO, Denso e Hier√°rquico contendo APENAS as veda√ß√µes, proibi√ß√µes e condutas permitidas/restritas aos
    agentes p√∫blicos no per√≠odo eleitoral.
    
    ** Considere rigorosamente as condutas vedadas em fun√ß√£o das datas de 6 e 3 meses que antecedem o pleito, pois, 
    h√° veda√ß√µes como propaganda institucional, uso de bens p√∫blicos, etc, que dependem da data_referencia.

    Estrutura obrigat√≥ria do resumo (use exatamente este formato markdown para facilitar parsing):
    - **Veda√ß√µes principais** (liste com bullets numerados ou -)
    - **Per√≠odo de incid√™ncia** (datas relativas √† elei√ß√£o)
    - **Exce√ß√µes e condutas permitidas**
    - **San√ß√µes t√≠picas** (breve)

    A an√°lise n√£o deve prejudicar a compreens√£o do conte√∫do legal, por isso, al√©m de completo, deve ser
     o mais fiel poss√≠vel ao texto original, mas eliminando redund√¢ncias e linguagem prolixa.
     
    Deixe bem claras as veda√ß√µes correspondentes aos prazos de 3 e 6 meses que antecedem o pleito (data_referencia).

    Base legal completa:
    \"\"\"{base_legal}\"\"\"

    Responda APENAS com o documento da an√°lise estruturada, sem introdu√ß√£o nem conclus√£o.
    """
    messages = [ChatCompletionUserMessageParam(role="user", content=base_legal)]

    try:
        response = client.chat.completions.create(
            model=modeloIA,
            messages=messages,
            temperature=0.1,  # baixa criatividade para fidelidade
            max_tokens=1000
        )
        print(response)
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.warning(f"Erro ao resumir base legal: {e}")
        return base_legal[:8000] + " [resumo truncado devido a erro]"

if "conteudo_base_legal" not in st.session_state:
    st.session_state.conteudo_base_legal = ""

st.markdown("### **Base Legal**")
with st.expander("üìã Base Legal", expanded=False):
    st.markdown("Defina o texto de refer√™ncia legal que ser√° usado na an√°lise de conformidade pelo LLM.")

    # Carregar m√∫ltiplos TXT como refer√™ncia

    st.markdown("### Upload arquivos .txt")
    st.markdown("**Carregue at√© 2 arquivos .txt** com trechos da lei, resolu√ß√£o, portaria, cartilha etc.")

    uploaded_txt_files = st.file_uploader(
        "Selecione arquivos TXT",
        type=["txt"],
        accept_multiple_files=True,
        key="txt_referencia_multi",
        help="M√°ximo de 2 arquivos. Todos ser√£o combinados em um √∫nico texto para a an√°lise."
    )

    conteudo_base_legal_referencia = "" #declara como str

    if uploaded_txt_files:
        if len(uploaded_txt_files) > 2:
            st.error("Limite m√°ximo: 2 arquivos TXT.")
            uploaded_txt_files = uploaded_txt_files[:2]

        textos_carregados = []
        for file in uploaded_txt_files:
            try:
                contenteudo_txt = file.read().decode("utf-8") # carrega o arquivo com conte√∫do normativo .txt na vari√°vel
                # junta os conte√∫do para formar a base legal
                textos_carregados.append(f"\n\n=== Conte√∫do de: {file.name} ===\n{contenteudo_txt}") #lista de conte√∫dos
            except Exception as e:
                st.warning(f"Erro ao ler {file.name}: {e}")

        if textos_carregados:
            conteudo_base_legal_referencia = "\n".join(textos_carregados) #transfoma a lista textos_carregados em um s√≥ conte√∫do
            st.success(f"{len(textos_carregados)} arquivo(s) TXT carregado(s) com sucesso.")
            st.caption(f"Total de caracteres: {len(conteudo_base_legal_referencia):,}")

        # Campo opcional para texto manual
        st.markdown("**Ou cole texto diretamente (opcional)**")
        texto_manual = st.text_area(
            "Texto adicional ou complementar.",
            height=150,
            placeholder="Cole aqui trechos espec√≠ficos de julgados, artigos, doutrina etc."
        )

        # Texto final consolidado para a LLM
        if conteudo_base_legal_referencia or texto_manual.strip():
            st.session_state.conteudo_base_legal = conteudo_base_legal_referencia
            if texto_manual.strip():
                st.session_state.conteudo_base_legal += "\n\n" + texto_manual.strip()
            st.info("Texto de refer√™ncia pronto.")

            if st.button("Gerar Resumo da Base Legal"):
                with st.spinner("Resumindo base legal..."):
                    resumo = resumir_base_legal(
                        st.session_state.conteudo_base_legal,
                        st.session_state.data_referencia.strftime('%d/%m/%Y') if st.session_state.data_referencia else "n√£o informada",
                        modeloIA
                    )
                    st.session_state.resumo_base_legal = resumo
                    st.success("Resumo gerado!")
                    st.markdown("**Resumo gerado:**")
                    st.markdown(resumo)

        # print(conteudo_base_legal_referencia)

#  ----------------- FIM PROCESSAMENTO BASE LEGAL ---------------------------
# ==========================================================================##


# ==========================================================================##
#  ---------------------- IN√çCIO PROCESSAMENTO PROMPT AN√ÅLISE ----------------
# """


prompt_padrao = """
Voc√™ √© um jurista especializado em compliance, com larga experi√™ncia em Direito Administrativo, Direito Eleitoral e 
√©tica na Administra√ß√£o P√∫blica Federal.

Atue de forma t√©cnica, objetiva, fundamentada e neutra, sem emitir ju√≠zos pol√≠ticos ou valorativos.
[/PERSONA]

[CONTEXTO]
Durante o per√≠odo eleitoral, √© essencial que a Administra√ß√£o P√∫blica observe rigorosamente as normas legais e √©ticas aplic√°veis
√†s comunica√ß√µes institucionais, bem como as condutas que s√£o vedadas por lei, regulamento, norma etc. 

Para fins desta an√°lise de conformidade, s√£o considerados, EXCLUSIVAMENTE: 
1 - O texto passado pelo usu√°rio por meio da vari√°vel "texto";
2 - a data do pleito passada por meio da vari√°vel "data_referencia"; e 
3 - O RESUMO PR√âVIO DA BASE LEGAL processado na etapa resumo da base legal.

[FLUXO]
Com base no texto, execute rigorosamente as seguintes etapas: 
1 - Divida o texto abaixo em trechos significativos (frases ou par√°grafos com ideia completa e aut√¥noma).
2 - Analise a conformidade de cada trecho com rela√ß√£o ao RESUMO PR√âVIO DA BASE LEGAL.
3 - Observe rigorosamente a data de in√≠cio do pleito e as veda√ß√µes correspondentes aos per√≠odos de 3 e 6 meses que antecedem o pleito. As regras est√£o 
na resultado do processamento da base legal. 

RESUMO DA BASE LEGAL (refer√™ncia √∫nica para julgar conformidade):
\"\"\"{resumo_base_legal}\"\"\"

INSTRU√á√ïES RESTRI√á√ÉO SOBRE ELEMENTOS OU TAGs DE CONTE√öDOS EXTRA√çDOS ‚Äì Desconsidere trechos cujo header traz uma dos seguintes termos:
- Ignore completamente links ou trechos que iniciem ou contenha de forma estrutural do html os seguintes termos: 
  'pol√≠tica de privacidade', 'cookies', 'LGPD', 'acessibilidade', 
  'navega√ß√£o' '(TAB/ENTER/CTRL)', 'raz√£o social', 'CNPJ', 'endere√ßo', 'termos de uso', 'login'', 
  'contato', 'rodap√©', 'menu', 'header',  'footer', "Acesse", "Servi√ßos", "√ìrg√£o Vinculado", "Siga-nos" ou 
   qualquer elemento estrutural que n√£o seja um texto com n√£o-not√≠cia.
  
- Foque apenas em not√≠cias, comunicados ou textos institucionais relevantes.
- Divida o texto em trechos significativos (frases ou par√°grafos com ideia completa).
- Classifique cada trecho como "conforme" ou "n√£o_conforme" com base no resumo.
- N√ÉO escreva NENHUM texto explicativo, introdu√ß√£o, conclus√£o, coment√°rio ou palavra extra.
- Retorne EXATAMENTE cada trecho analisado para o processo de contagem, 
  sem aspas extras, sem JSON, sem formata√ß√£o adicional.
_ Para cada trecho n√£o conforme adicione √† lista trechos_nao_conformes

---------------------- RESULTADO ---------------------------------

A resposta final tem apenas 2 vari√°veis, trechos_nao_conformes e contagem, e deve-se seguir rigorosamente os seguintes formatos:

trechos_nao_conformes = [["trecho1 n√£o conforme"], ["trecho2 n√£o conforme"], ...]

contagem = [total_trechos_analisados, total_conformes, total_nao_conformes]

Exemplos obrigat√≥rios do formato exato (copie exatamente):
Se houver 2 n√£o conformes em 10 trechos (8 conformes):
trechos_nao_conformes = [["Texto do primeiro trecho n√£o conforme"], ["Texto do segundo trecho n√£o conforme"]]
contagem = [10, 8, 2]


Texto para an√°lise:
\"\"\"{texto}\"\"\"

Data de refer√™ncia:
\"\"\"{data_referencia}\"\"\"

Responda SOMENTE com as duas linhas acima. Nada mais.
"""

st.markdown("### **Prompt**" )
with st.expander("üß† Prompt", expanded=False):

    st.markdown("#### Prompt para An√°lise")

    if "prompt_reset" not in st.session_state:
        st.session_state.prompt_reset = 0

    prompt_personalizado = st.text_area(
        "Edite o prompt que ser√° enviado ao modelo",
        value=prompt_padrao, # a vari√°vel prompt_personalizado recebe o conte√∫do do prompt_padrao, alterado ou n√£o
        height=350,
        key=f"prompt_editor_{st.session_state.prompt_reset}"
    )

#  ----------------- FIM PROCESSAMENTO PROMPT -------------------------------
# ==========================================================================##

st.divider()


# ============================================================
#  FUN√á√ÉO PARA COLETA DE LINKS DO SITE (ok)
# ============================================================

def coletar_links_internos(url: str, max_links) -> set:
    downloaded = trafilatura.fetch_url(url)  # web scraping
    if not downloaded:
        return {url}
    try:
        tree = html.fromstring(downloaded) # converte em uma √°rvore de dados hier√°rquicos
    except Exception:
        return {url}

    dominio = urlparse(url).netloc # extrai a parte da rede de uma URL
    links_validos = {url}

    #Loop para interar sobre todos os atributos href das tags de √¢ncora (<a>) do tree.
    for href in tree.xpath("//a/@href"): #
        full = urljoin(url, href.strip())
        parsed = urlparse(full)

        if parsed.netloc != dominio: # Verifica se o dom√≠nio da URL extra√≠da √© o mesmo que o dom√≠nio da p√°gina original
            continue                 # se for diferente, ignora o link e n√£o coleta o link externo.

        path = parsed.path.lower()

        if any(block in path for block in LISTA_1): # se verdadeiro ignora e n√£o coleta o link
            continue

        if re.search(r'\.(pdf|jpg|jpeg|png|gif|zip|docx?|xlsx?)$', path): # se verdadeiro ignora e n√£o coleta o link
            continue

        links_validos.add(full)

        if len(links_validos) >= max_links:
            break

    return links_validos

# ============================================================
#          FUN√á√ÉO PARA EXTRA√á√ÉO DE TEXTO     (ok)
# ============================================================

@st.cache_data(ttl=3600)
def extrair_texto(url_noticia: str) -> str:
    # 1. BAIXA HTML BRUTO ‚Äî ESSENCIAL (ok)

    downloaded_noticia = trafilatura.fetch_url(url_noticia)

    if not downloaded_noticia:
        print(f"[ERRO] Falha ao baixar HTML bruto: {url_noticia}")
        return ""

    texto_final = None

    # 2. PRIMEIRA TENTATIVA ‚Äî Trafilatura com m√°ximo recall

    try:
        text_noticia = trafilatura.extract(
            downloaded_noticia,
            include_comments=False,
            include_images=False,
            include_tables=False,
            deduplicate=True,
            favor_recall=True,
            favor_precision=True,
            no_fallback=False,
            include_formatting=False,
            output_format="txt",
        )

        if text_noticia and len(text_noticia.strip()) > 150:
            texto_final = text_noticia # retorna uma str
        else:
            print(f"[WARN] Extra√ß√£o Trafilatura baixa em {url_noticia}")

    except Exception as e:
        print(f"[ERRO Trafilatura] {url_noticia}: {e}")

    # 3. FALLBACK 1 ‚Äî html2txt (Trafilatura modo bruto)

    if not texto_final:
        try:
            print(f"[FALLBACK] html2txt ativado para {url_noticia}")
            raw_text = html2txt(downloaded_noticia)
            if raw_text and len(raw_text.strip()) > 150:
                texto_final = raw_text
        except:
            pass

    # 4. FALLBACK 2 ‚Äî BeautifulSoup (captura TODO texto vis√≠vel)

    if not texto_final:
        try:
            print(f"[FALLBACK] BeautifulSoup ativado para {url_noticia}")
            bs_noticia = BeautifulSoup(downloaded_noticia, "lxml")

            # Remove scripts, styles etc.
            for tag in bs_noticia(["script", "style", "noscript"]):
                tag.extract()

            bs_text = bs_noticia.get_text(separator="\n")
            if bs_text and len(bs_text.strip()) > 150:
                texto_final = bs_text

        except Exception as e:
            print(f"[ERRO BS4] {url_noticia}: {e}")

    if not texto_final:
        print(f"[ERRO] Nenhum m√©todo conseguiu extrair texto de {url_noticia}")
        return ""

    # print("texto extra√ß√£o")
    # print(texto_final)

    return texto_final

##### Esse trecho foi mantido para eventuais testes de raspagem feitos de forma mais crua.
# def extrair_texto(url: str) -> str:
#     # Baixa o HTML bruto
#     html = trafilatura.fetch_url(url)
#     if not html:
#         print(f"[ERRO] N√£o foi poss√≠vel baixar: {url}")
#         return ""
#
#     # Extrai o texto no modo padr√£o
#     texto_final = trafilatura.extract(html)
#
#     return texto_final if texto_final else ""


# ==============================================================
#            FUN√á√ÉO PARA FILTRAR CONTE√öDO IRRELEVANTE
# ==============================================================

def filtrar_conteudo_relevante(texto: str) -> str:
    if not texto:
        return ""
    irrelevantes_keywords = [
        "pol√≠tica de privacidade", "cookies", "lgpd", "acessibilidade", "navega√ß√£o", "teclas", "tab", "enter",
        "rolagem", "ctrl", "command", "raz√£o social", "cnpj", "endere√ßo", "contato", "login", "termos de uso",
        "sobre n√≥s", "rodap√©", "footer", "header", "menu", "navegador", "privacidade", "seguran√ßa", "captcha",
        "WhatsApp"
    ]
    # Remove se√ß√µes inteiras que contenham palavras-chave
    blocos = re.split(r'\n\s*\n', texto)  # separa por par√°grafos duplos
    blocos_filtrados = []
    for bloco in blocos:
        if not any(kw.lower() in bloco.lower() for kw in irrelevantes_keywords): # o que n√£o est√° em bloco irrelevante passa.
            blocos_filtrados.append(bloco)
    return "\n\n".join(blocos_filtrados).strip()


# ============================================================
#  FUN√á√ÉO PARA AN√ÅLISE COM LLM - chamada da API do Groq (ok)
# ============================================================

def analisar_com_llm(texto: str,
                     model: str,
                     temperatura: float,
                     prompt_personalizado: str,
                     data_referencia):

    texto_filtrado = filtrar_conteudo_relevante(texto)
    if not texto_filtrado:
        return [], [0, 0, 0]

    data_ref_str = data_referencia.strftime('%d/%m/%Y') if data_referencia else "n√£o informada"

    try:
        prompt_completo = prompt_personalizado.format(
            texto=texto_filtrado,
            data_referencia=data_ref_str,
            resumo_base_legal=st.session_state.get("resumo_base_legal", "Nenhum resumo dispon√≠vel")
        )
    except Exception as e:
        st.error(f"Erro no formato do prompt: {e}")
        return [], [0, 0, 0]

    try:
        messages = [ChatCompletionUserMessageParam(role="user", content=prompt_completo)]
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperatura,
            max_tokens=800
        )

        content = response.choices[0].message.content.strip()
        print("=====================================content========================")
        print(content)

        trechos_nao_conformes = []
        contagem = [0, 0, 0]

        # Modifica√ß√£o 1: Express√£o regular mais flex√≠vel
        match_trechos = re.search(r'trechos_nao_conformes\s*=\s*(\[.*?\])', content, re.DOTALL | re.IGNORECASE)
        if match_trechos:
            lista_str = match_trechos.group(1)
            # Limpar aspas e caracteres especiais
            lista_str = lista_str.replace('‚Äú', '"').replace('‚Äù', '"').replace("'", '"')
            # Remover quebras de linha dentro das strings
            lista_str = re.sub(r'\n', ' ', lista_str)
            try:
                lista_trechos = json.loads(lista_str)
                # Extrair strings das listas internas
                trechos_nao_conformes = []
                for item in lista_trechos:
                    if isinstance(item, list) and len(item) > 0:
                        trechos_nao_conformes.append(str(item[0]).strip())
                    elif isinstance(item, str):
                        trechos_nao_conformes.append(item.strip())
            except json.JSONDecodeError as e:
                print("Erro ao parsear trechos:", e, "\nConte√∫do bruto:", lista_str)
                # Fallback: tentar extrair manualmente
                padrao_fallback = r'\[\s*"([^"]+)"\s*\]'
                trechos_encontrados = re.findall(padrao_fallback, lista_str)
                if trechos_encontrados:
                    trechos_nao_conformes = [t.strip() for t in trechos_encontrados]

        # Modifica√ß√£o 2: Express√£o regular para contagem
        match_contagem = re.search(r'contagem\s*=\s*(\[\s*\d+\s*,\s*\d+\s*,\s*\d+\s*\])', content, re.IGNORECASE)
        if match_contagem:
            try:
                contagem_str = match_contagem.group(1)
                contagem = json.loads(contagem_str)
            except:
                print("Erro ao parsear contagem:", match_contagem.group(1))
                # Fallback: extrair n√∫meros
                numeros = re.findall(r'\d+', contagem_str)
                if len(numeros) >= 3:
                    contagem = [int(n) for n in numeros[:3]]

        return trechos_nao_conformes, contagem

    except Exception as e:
        st.warning(f"Erro na chamada ao LLM: {e}")
        return [], [0, 0, 0]


# ========================================================================================
# --------------------------------AN√ÅLISE DOS SITES --------------------------------------
# ========================================================================================

if "resultados" not in st.session_state:
    st.session_state.resultados = []

colAnalisar1, colAnalisar2, colAnalisar3 = st.columns([1, 2, 1])
with colAnalisar2:
    analisar = st.button("üöÄ **Analisar Sites**", type="primary", use_container_width=True)

if analisar:
    if st.session_state.sites_df.empty:
        st.error("Adicione pelo menos um site antes de analisar.")
    else:
        sites = st.session_state.sites_df.to_dict("records")  # ok
        resultados_analise_llm = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        # print(sites)
        for idx, site in enumerate(sites):
            url = site["URL"]
            status_text.text(f"Analisando {idx + 1}/{len(sites)}: {url}")
            # print(max_links)
            links = coletar_links_internos(url, max_links=max_links)

            total_trechos = 0
            total_conformes = 0
            total_nao_conformes = 0
            trechos_nao_conformes = []


            # print(links)
            for link in links:
                texto = extrair_texto(link)
                if texto:
                    trechos_nao_conformes_site, lista_contagem = analisar_com_llm(
                        texto,
                        modeloIA,
                        temperatura,
                        prompt_personalizado,
                        st.session_state.data_referencia
                    )
                    # Acumula os trechos (lista de strings)
                    trechos_nao_conformes.extend(trechos_nao_conformes_site)

                    # Acumula contagens
                    total_trechos += lista_contagem[0]
                    total_conformes += lista_contagem[1]
                    total_nao_conformes += lista_contagem[2]

            # Calcula percentual de conformidade da URL
            if total_trechos == 0:
                perConformes = 0.0
            else:
                perConformes = round((total_conformes / total_trechos) * 100, 1)

            resultados_analise_llm.append({

                "url": url,
                "conformidade": perConformes,
                "total_trechos": total_trechos,
                "conformes": total_conformes,
                "nao_coformes": total_nao_conformes,
                "trechos_nao_conformes": trechos_nao_conformes

            })
            print("_____________________resultados_analise_llm___________________")
            print(resultados_analise_llm)

            progress_bar.progress((idx + 1) / len(sites))

        status_text.empty()
        progress_bar.empty()
        st.session_state.resultados = resultados_analise_llm

# =====================================================================
# ========================= GR√ÅFICO DE BARRAS =========================
# =====================================================================

resultados_para_plot = st.session_state.get("resultados", [])

if resultados_para_plot:
    def nome_grafico(url):
        return extrair_subdominio_gov(url)


    df_result = pd.DataFrame({
        "Site": [nome_grafico(r.get("url", "")) for r in resultados_para_plot],
        "Conformidade (%)": [float(r.get("conformidade", 0.0)) for r in resultados_para_plot]
        # chave correta √© "conformidade"
    })

    df_result = df_result.dropna(subset=["Site", "Conformidade (%)"])

    trechos_nao_conformes = []

    for resultado in resultados_para_plot:
        url = resultado.get("url", "‚Äî")
        nome_site = nome_grafico(url)
        trechos = resultado.get("trechos_nao_conformes", [])  # lista de strings

        for trecho in trechos:
            if isinstance(trecho, str) and trecho.strip():
                trechos_nao_conformes.append({
                    "Site": nome_site,
                    "Trecho": trecho.strip(),
                    "Classifica√ß√£o": "nao_conforme",
                    "URL original": url
                })

    if trechos_nao_conformes:
        df_nao_conformes = pd.DataFrame(trechos_nao_conformes)

        st.divider()
        st.subheader("üü• Trechos identificados como poss√≠vel ind√≠cio de conduta vedada")

        # Exibe a tabela interativa (com filtro, ordena√ß√£o, etc.)
        st.dataframe(
            df_nao_conformes,
            column_config={
                "Site": st.column_config.TextColumn("Site", width="medium"),
                "Trecho": st.column_config.TextColumn("Trecho identificado", width="large"),
                "Classifica√ß√£o": st.column_config.TextColumn("Classif.", width="small"),
                "URL original": st.column_config.LinkColumn("URL", width="medium", display_text=r"https?://(.+)")
            },
            hide_index=True,
            use_container_width=True
        )

        # contador r√°pido trechos
        st.caption(f"Total de trechos com n√£o conformidades: **{len(df_nao_conformes)}**")

        # Bot√£o para baixar CSV
        csv = df_nao_conformes.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üì• Baixar tabela como CSV",
            data=csv,
            file_name="trechos_indicio.csv",
            mime="text/csv"
        )
    else:
        st.info("Nenhum trecho classificado como 'n√£o conforme' foi encontrado na an√°lise.")

    if not df_result.empty:
        col_esq, col_centro, col_dir = st.columns([1, 2, 1])

        with col_centro:
            fig, ax = plt.subplots(figsize=(10, 5))

            sites = df_result["Site"]
            valores = df_result["Conformidade (%)"].astype(float).clip(0, 100)

            # Cores por gradiente
            cores = plt.colormaps['viridis'](valores / 100.0)

            bars = ax.bar(sites, valores, color=cores, edgecolor='blue', linewidth=0.8)

            # R√≥tulos com percentual
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width() / 2,
                    height + 1,
                    f'{height:.1f}%',
                    ha='center',
                    va='bottom',
                    fontsize=8,
                    fontweight='bold'
                )

            ax.set_xlabel("")
            ax.set_ylabel("Conformidade (%)", fontsize=10)
            ax.set_title(" üìä Porcentagem de Conformidade", fontsize=10, pad=20)

            ax.tick_params(axis='x', labelsize=8, rotation=45)
            ax.tick_params(axis='y', labelsize=8)

            ax.grid(axis='y', linestyle='--', alpha=0.4)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)

            ax.set_ylim(0, 100)

            plt.tight_layout()
            st.pyplot(fig)

# Rodap√©
st.markdown("---")
st.caption("Analisador de Conformidade de Conduta Vedada | Desenvolvido por Fabiana, Jo√£o Vicente, L√≠via, T√∫lio e Yro√°")
